#! /usr/bin/env python
""" Call variantss and InDels from the input FASTQ. """
import json
import sys
from os.path import dirname

from ruffus import *

from call_variants.helpers.time_job import time_job
from call_variants.tasks import variants, shared

parser = cmdline.get_argparse(description='Call variants from input FASTQ.')
parser.add_argument("fastq", metavar="INPUT_FASTQ", help="Input FASTQ file")
parser.add_argument("reference", metavar="REFERENCE_FASTA",
                    help="Input reference FASTA file.")
parser.add_argument("genbank", metavar="REFERENCE_GENBANK",
                    help="Input reference GenBank file.")
parser.add_argument("-r", "--read_length", dest="read_length", type=int,
                    help="Mean read length of input FASTQ file", metavar="INT",
                    default=0)
parser.add_argument('-p', '--processors', metavar="INT", type=int, default=1,
                    help='Number of processors to use. (Default 1)')
parser.add_argument('--tag', help='Prefix for final VCF. (Default variants)',
                    dest='tag', default='variants')
parser.add_argument('--log_times', action='store_true', default=False,
                    help='Write task run times to file (Default: STDERR).', )
options = parser.parse_args()

TIME_LOG = 'logs/call_variants.time' if options.log_times else sys.stderr
# Pipeline --------------------------------------------------------------------
read_length = 0
if not options.read_length:
    # Try to read a stats file
    try:
        stats = options.fastq.replace('gz', 'stats')
        with open(stats, 'r') as f:
            json_data = json.loads(f.readline().rstrip())
            read_length = json_data['mean_read_length']
    except:
        # Unknown read length, set to 0 and use BWA aln/samse
        read_length = 0
else:
    read_length = options.read_length


@active_if(options.log_times)
def create_dir():
    """ Make logs directory if required. """
    out, err = shared.run_command(['mkdir', 'logs'])


@mkdir('variants/completed')
@mkdir('variants/gatk')
@transform(options.reference, regex(r"(.*)"), r"\1.SOME")
def build_index(input_file, completed_file):
    """ Align reads using BWA. """
    variants.bwa_index(options.reference)


@time_job(TIME_LOG, new_stream=True)
def bwa(input_file, completed_file):
    """ Align reads using BWA. """
    output_sam = completed_file.replace('completed', 'gatk')

    if (read_length > 70):
        # Run BWA mem
        variants.bwa_mem(options.fastq, output_sam, str(options.processors),
                         options.reference, completed_file)
    else:
        # Run BWA aln/samse
        sai = output_sam.replace('sam', 'sai')
        variants.bwa_aln(options.fastq, sai, output_sam,
                         str(options.processors), options.reference,
                         completed_file)


@follows(bwa)
@transform(bwa, regex(r"(.*)"), r"variants/completed/sorted.bam")
@time_job(TIME_LOG)
def add_or_replace_read_groups(input_file, completed_file):
    """ Filter out reads. """
    input_sam = input_file.replace('completed', 'gatk')
    sorted_bam = completed_file.replace('completed', 'gatk')
    variants.add_or_replace_read_groups(input_sam, sorted_bam, completed_file)


@follows(add_or_replace_read_groups)
@transform(add_or_replace_read_groups, regex(r"(.*)"),
           r"variants/completed/deduped.bam")
@time_job(TIME_LOG)
def mark_duplicates(input_file, completed_file):
    """ Build BAM index. """
    sorted_bam = input_file.replace('completed', 'gatk')
    deduped_bam = completed_file.replace('completed', 'gatk')
    variants.mark_duplicates(sorted_bam, deduped_bam, completed_file)


@follows(mark_duplicates)
@transform(mark_duplicates, regex(r"(.*).bam"), r"\1.intervals")
@time_job(TIME_LOG)
def realigner_target_creator(input_file, completed_file):
    """ Check Tauqeer's protocol. """
    deduped_bam = input_file.replace('completed', 'gatk')
    intervals = completed_file.replace('completed', 'gatk')
    variants.realigner_target_creator(deduped_bam, intervals,
                                      optiona.reference, completed_file)


@follows(realigner_target_creator)
@transform(realigner_target_creator, regex(r"(.*)"),
           r"variants/completed/realigned.bam")
@time_job(TIME_LOG)
def indel_realigner(input_file, completed_file):
    """ Check Tauqeer's protocol. """
    intervals = input_file.replace('completed', 'gatk')
    deduped_bam = intervals.replace('intervals', 'bam')
    realigned_bam = completed_file.replace('completed', 'gatk')
    variants.indel_realigner(intervals, deduped_bam, realigned_bam,
                             options.reference, completed_file)


@follows(indel_realigner)
@transform(indel_realigner, regex(r"(.*)"), r"variants/completed/raw.vcf")
@time_job(TIME_LOG)
def haplotype_caller(input_file, completed_file):
    """ Check Tauqeer's protocol. """
    realigned_bam = input_file.replace('completed', 'gatk')
    output_vcf = completed_file.replace('completed', 'gatk')
    variants.haplotype_caller(realigned_bam, output_vcf, options.reference,
                              completed_file)


@follows(haplotype_caller)
@transform(haplotype_caller, regex(r"(.*)"),
           r"variants/completed/filtered.vcf")
@time_job(TIME_LOG)
def variant_filtration(input_file, completed_file):
    """ Check Tauqeer's protocol. """
    input_vcf = input_file.replace('completed', 'gatk')
    filtered_vcf = completed_file.replace('completed', 'gatk')
    variants.variant_filtration(input_vcf, filtered_vcf, options.reference,
                                completed_file)


@follows(variant_filtration)
@transform(variant_filtration, regex(r"(.*)"),
           r"variants/completed/annotated.vcf")
@time_job(TIME_LOG)
def vcf_annotator(input_file, completed_file):
    """ Annotate the variants. """
    filtered_vcf = input_file.replace('completed', 'gatk')
    annotated_vcf = completed_file.replace('completed', 'gatk')
    variants.vcf_annotator(filtered_vcf, annotated_vcf, options.genbank,
                           completed_file)


@follows(vcf_annotator)
@transform(vcf_annotator, regex(r"(.*)"), r"\1.gz")
@time_job(TIME_LOG)
def move_final_vcf(input_file, completed_file):
    """ Move the final annotated VCF to the root of the project. """
    annotated_vcf = input_file.replace('completed', 'gatk')
    final_vcf = '{0}.variants.vcf.gz'.format(options.tag)
    variants.move_final_vcf(annotated_vcf, final_vcf, completed_file)


@follows(move_final_vcf)
@transform(move_final_vcf, regex(r"(.*).vcf.gz"), r"\1.cleanup")
@time_job(TIME_LOG)
def cleanup(input_file, completed_file):
    """ Remove all the intermediate files. """
    base_dir = dirname(input_file.replace('completed', 'gatk'))
    tar_gz = '{0}/gatk/gatk.tar.gz'.format(dirname(base_dir))
    variants.cleanup(base_dir, tar_gz, completed_file)

# -----------------------------------------------------------------------------
pipeline_run(exceptions_terminate_immediately=True, verbose=5)
